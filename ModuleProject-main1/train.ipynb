{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class HandGestureRecognition:\n",
    "    def __init__(self, model_path, actions, seq_length=30):\n",
    "        self.actions = actions\n",
    "        self.seq_length = seq_length\n",
    "        self.model = load_model(model_path)\n",
    "\n",
    "        # Initialize MediaPipe Hands\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "        # Initialize variables\n",
    "        self.seq = []\n",
    "        self.action_seq = []\n",
    "        \n",
    "    def preprocess_frame(self, frame):\n",
    "        flipped_frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(flipped_frame, cv2.COLOR_BGR2RGB)\n",
    "        return rgb_frame\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        result = self.hands.process(frame)\n",
    "        return result\n",
    "\n",
    "    def extract_joint_angles(self, hand_landmarks):\n",
    "        joint = np.zeros((21, 4))\n",
    "        for j, lm in enumerate(hand_landmarks):\n",
    "            joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "\n",
    "        v1 = joint[[0, 1, 2, 3, 0, 5, 6, 7, 0, 9, 10, 11, 0, 13, 14, 15, 0, 17, 18, 19], :3]  # Parent joints\n",
    "        v2 = joint[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], :3]  # Child joints\n",
    "        v = v2 - v1\n",
    "        v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "        angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                                    v[[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 16, 17, 18], :],\n",
    "                                    v[[1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 17, 18, 19], :]))\n",
    "        angle = np.degrees(angle)\n",
    "        return np.concatenate([joint.flatten(), angle])\n",
    "\n",
    "    def predict_action(self, input_data):\n",
    "        input_data = np.expand_dims(np.array(input_data, dtype=np.float32), axis=0)\n",
    "        y_pred = self.model.predict(input_data).squeeze()\n",
    "        i_pred = int(np.argmax(y_pred))\n",
    "        conf = y_pred[i_pred]\n",
    "        return i_pred, conf\n",
    "\n",
    "    def draw_landmarks(self, frame, hand_landmarks):\n",
    "        self.mp_drawing.draw_landmarks(frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    def run(self):\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, img = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            img_rgb = self.preprocess_frame(img)\n",
    "            result = self.process_frame(img_rgb)\n",
    "\n",
    "            if result.multi_hand_landmarks:\n",
    "                for res in result.multi_hand_landmarks:\n",
    "                    d = self.extract_joint_angles(res.landmark)\n",
    "                    self.seq.append(d)\n",
    "\n",
    "                    if len(self.seq) < self.seq_length:\n",
    "                        continue\n",
    "\n",
    "                    input_data = self.seq[-self.seq_length:]\n",
    "                    i_pred, conf = self.predict_action(input_data)\n",
    "\n",
    "                    if conf < 0.9:\n",
    "                        continue\n",
    "\n",
    "                    action = self.actions[i_pred]\n",
    "                    self.action_seq.append(action)\n",
    "\n",
    "                    if len(self.action_seq) < 3:\n",
    "                        continue\n",
    "\n",
    "                    this_action = '?' if self.action_seq[-1] != self.action_seq[-2] or self.action_seq[-1] != self.action_seq[-3] else action\n",
    "\n",
    "                    cv2.putText(img, f'{this_action.upper()}', \n",
    "                                org=(int(res.landmark[0].x * img.shape[1]), \n",
    "                                     int(res.landmark[0].y * img.shape[0] + 20)), \n",
    "                                fontFace=cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                                fontScale=1, \n",
    "                                color=(255, 255, 255), \n",
    "                                thickness=2)\n",
    "\n",
    "                    self.draw_landmarks(img, res)\n",
    "\n",
    "            cv2.imshow('Hand Gesture Recognition', img)\n",
    "\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    actions = ['good', 'five', 'fit', 'V', 'piece']\n",
    "    model_path = r'C:\\Users\\user\\Desktop\\python\\ModuleProject-main\\model.keras'\n",
    "    recognizer = HandGestureRecognition(model_path, actions)\n",
    "    recognizer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/user/Desktop/python/ModuleProject-main/dataset/seq_seq_fit_1736319783.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m good_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/user/Desktop/python/ModuleProject-main/dataset/seq_good_1736319783.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m five_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/user/Desktop/python/ModuleProject-main/dataset/seq_five_1736319783.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m fit_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/user/Desktop/python/ModuleProject-main/dataset/seq_seq_fit_1736319783.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m V_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/user/Desktop/python/ModuleProject-main/dataset/seq_V_17363197.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m piece_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/user/Desktop/python/ModuleProject-main/dataset/seq_piece_1736319783.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/user/Desktop/python/ModuleProject-main/dataset/seq_seq_fit_1736319783.npy'"
     ]
    }
   ],
   "source": [
    "good_data = np.load('C:/Users/user/Desktop/python/ModuleProject-main/dataset/seq_good_1736319783.npy')\n",
    "five_data = np.load('C:/Users/user/Desktop/python/ModuleProject-main/dataset/seq_five_1736319783.npy')\n",
    "fit_data = np.load('C:/Users/user/Desktop/python/ModuleProject-main/dataset/seq_seq_fit_1736319783.npy')\n",
    "V_data = np.load('C:/Users/user/Desktop/python/ModuleProject-main/dataset/seq_V_17363197.npy')\n",
    "piece_data = np.load('C:/Users/user/Desktop/python/ModuleProject-main/dataset/seq_piece_1736319783.npy')\n",
    "\n",
    "print(\"Good shape:\", good_data.shape)\n",
    "print(\"Five shape:\", five_data.shape)\n",
    "print(\"Fit shape:\", fit_data.shape)\n",
    "print(\"V shape:\", V_data.shape)\n",
    "print(\"Piece shape:\", piece_data.shape)\n",
    "\n",
    "# Concatenate data\n",
    "data = np.concatenate([good_data, five_data, fit_data, V_data, piece_data], axis=0)\n",
    "print(\"Concatenated data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=2021)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LeakyReLU\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(64, input_shape=x_train.shape[1:3], return_sequences=False),  # GRU 레이어\n",
    "    LeakyReLU(alpha=0.3),  # LeakyReLU 적용\n",
    "    Dropout(0.5),  # 50% 드롭아웃\n",
    "    Dense(32),\n",
    "    LeakyReLU(alpha=0.3),  # LeakyReLU 적용\n",
    "    Dropout(0.3),  # 30% 드롭아웃\n",
    "    Dense(len(actions), activation='softmax')  # 출력층\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=200,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/model.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "\n",
    "multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
